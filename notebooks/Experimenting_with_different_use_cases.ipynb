{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaoDr57T74ag"
      },
      "source": [
        "# 1 - Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OXC3dguV3cDb",
        "outputId": "8bf49370-2584-4f75-d49f-06bc2a6ab528"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m27.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.0.1\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWLw1TOn6P_u"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel, TFAutoModel, AutoConfig\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from transformers import TFAutoModelForSequenceClassification\n",
        "from transformers import pipeline\n",
        "import numpy as np\n",
        "from scipy.spatial.distance import cosine\n",
        "from collections import defaultdict\n",
        "import urllib\n",
        "import numpy as np\n",
        "from scipy.special import softmax\n",
        "from sklearn.metrics import classification_report"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1mUmexGY8FUr"
      },
      "source": [
        "# 2 - Fetch XLM-T model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8v8HPwj6P_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301,
          "referenced_widgets": [
            "9172a9416d4e4e18ad7c2fba4611f16c",
            "1ac481a0120b4492a708c39d44378011",
            "f761d5f51eff489c8797175221e9766e",
            "861f7450d2924d629145999aa468bb2e",
            "1d4ed9f120fb4f61828d0e351dc0149e",
            "8ad1cd330cab447ab713aae97f5f18ce",
            "5a0a5e15a96d4c599d078769c7902342",
            "6c23bf9c6d7045d89558b85c7a969cdd",
            "9cdbaae872a34417abd05c7c7bdeb9d0",
            "eecd656c35f34e2ab550e9dcec90945d",
            "8765dce74f464f878379bb9e97f5a0bd",
            "066340c7768e47818a3bbebc394f7026",
            "4fab715624b4492b9e7c000d4f4383b9",
            "e4a8825f70044b79a1fdcb5295a5d02c",
            "6f0cb33d2e4e45838ed194a74635571e",
            "fd121e8797ba4fdcac41223d38e3ad06",
            "c535c0bc3ae94fc4962a7254ae13e7d4",
            "cd9f6b7b836b45b691d2c70458458966",
            "42502e4dccc84b5cb001c718df8356b3",
            "0ab806dd9f89493eace1d944e7a35253",
            "c5711b4ed27d4ae0895a14018fc511ac",
            "7112180b4e844412ac848c8f1a9c936f",
            "274910d4759e420ebc8e152438309013",
            "d032350872ae4adb9ccbb0ce5d28380e",
            "9d256f21d42a47ea9460bd0ec61175d4",
            "35b2f362429841e597740ebeca4606de",
            "a380cdd36b204a159e75ff0722a1d18e",
            "a98b6f5746564b3ab304f3e1e806585c",
            "2429b7ba01bc430eacf9608a760719fa",
            "d149620925ec4a10b7fd3450cef580df",
            "b5e6e00fb4584899a06ac6b0168f4595",
            "d7226490411141efa4ed48eb31726ebc",
            "6290423daadd43e99ce4f55ee193c21f",
            "a5ef892d56c64294bb8eb2d6a6a41a98",
            "dd28b496b9854b3d8162b79102350b99",
            "c55c30e1718b48b1a7c793e41c05dfab",
            "489b26f6302842538bda85213f7a717a",
            "7a4cba74397f43cd836a64d05141fcb6",
            "78fcc352dfbf47a081671887d4ebb97d",
            "14f6c5e124d14aeda36c2fa1fddb83be",
            "2e246988db9c4f1299ce342731572b91",
            "33080bdc1a3446cbb220a6118c517bd0",
            "b7ae535410744b0b9cd69c55c65c186c",
            "a1d2854656394bb99d193fdacfd95a7a"
          ]
        },
        "outputId": "053f7dda-b3a0-4e14-dac4-8417c9c22f58"
      },
      "source": [
        "MODEL =  \"cardiffnlp/twitter-xlm-roberta-base\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "model = AutoModel.from_pretrained(MODEL)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/652 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9172a9416d4e4e18ad7c2fba4611f16c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "066340c7768e47818a3bbebc394f7026"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/9.10M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "274910d4759e420ebc8e152438309013"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "a5ef892d56c64294bb8eb2d6a6a41a98"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of XLMRobertaModel were not initialized from the model checkpoint at cardiffnlp/twitter-xlm-roberta-base and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KS0tpy2FBETm"
      },
      "source": [
        "# Use Cases"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sNDGKawM-6ds"
      },
      "source": [
        "## 1 - Compute Tweet Similarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "collapsed": true,
        "id": "9uXVOKuE6P_z"
      },
      "source": [
        "def preprocess(text):\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    return \" \".join(new_text)\n",
        "\n",
        "def get_embedding(text):\n",
        "    text = preprocess(text)\n",
        "    encoded_input = tokenizer(text, return_tensors='pt')\n",
        "    features = model(**encoded_input)\n",
        "    features = features[0].detach().numpy()\n",
        "    features_mean = np.mean(features[0], axis=0)\n",
        "    return features_mean\n",
        "\n",
        "query = \"Acabo de pedir pollo frito 🐣\" #spanish"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gQh4qnTk3n2d"
      },
      "source": [
        "tweets = [\"We had a great time! ⚽️\", # english\n",
        "          \"We hebben een geweldige tijd gehad! ⛩\", # dutch\n",
        "          \"Nous avons passé un bon moment! 🎥\", # french\n",
        "          \"Ci siamo divertiti! 🍝\"] # italian\n",
        "\n",
        "d = defaultdict(int)\n",
        "for tweet in tweets:\n",
        "    sim = 1-cosine(get_embedding(query),get_embedding(tweet))\n",
        "    d[tweet] = sim"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQ45UcRq6P_0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07e2c92c-8731-4f73-88db-5506949998ba"
      },
      "source": [
        "print('Most similar to: ',query)\n",
        "print('----------------------------------------')\n",
        "for idx,x in enumerate(sorted(d.items(), key=lambda x:x[1], reverse=True)):\n",
        "  print(idx+1,x[0])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Most similar to:  Acabo de pedir pollo frito 🐣\n",
            "----------------------------------------\n",
            "1 Ci siamo divertiti! 🍝\n",
            "2 Nous avons passé un bon moment! 🎥\n",
            "3 We had a great time! ⚽️\n",
            "4 We hebben een geweldige tijd gehad! ⛩\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "heJ80Jc_EEYs"
      },
      "source": [
        "## 2 - Simple inference example (with `pipelines`)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m6iCJwB2EBfZ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 348,
          "referenced_widgets": [
            "1734ee4bd0dd425b8be9c8a69f5ce76b",
            "b36328f9b8434662a1d3745d2be127e1",
            "8d817aae2ee449f4afd416430ba830d3",
            "13969f0d02cf44b795b743aee835e62d",
            "d37e73c5f0ed4b9e95632cc1387107e3",
            "96b10ecb9b874f6999da6daf18f86aa4",
            "e7b4ff426bda4f62838f69c64f790a15",
            "a78489e4aa88469bb65f8b73092b6f98",
            "cd271340049f4033932830295bc5af22",
            "08b84dbc3d884504a75ecd0ef2d85862",
            "3c530da127f242c9bd00d5ba2059a254",
            "27ab61b2598e4998a38b28efb7e62db2",
            "21e1710827254e63900ffb0ca882bc12",
            "9d94a765448045fdab221e750c02b77c",
            "c821dcaf64ae40b4a38a0f709a1b40ac",
            "9d2ebc77f1794a6bae30e2a56b7768e8",
            "daeb5d748a6f4e658caf17986cb1fdef",
            "a7996b42d1764057aac5d6a14583003f",
            "ef780850ab744d6f93583f06de8aee9e",
            "663c08377865454b8ddba4f84be660c0",
            "5f8e285874a64798b5d213670c77ed39",
            "ea4b9d38f2de4f0c8e65407e84874f9c",
            "47a78b2da9b946838468386922e99a3b",
            "b3d9390410614edfb258d2d64f8fd2c7",
            "ef1d9198c94b46c48156093645067a55",
            "70b3acbd26b940399a3d7edeabb9c47c",
            "252f5d91a0af4127a698c70252a8f533",
            "92020916891840da96c1322574b372b9",
            "3b310ab410ac44f383ccfcfe071b7581",
            "c2c3d3f83ea94d4e874849180787a858",
            "ea24f2da9b32415f920128108ae36b61",
            "2f9c8f827c4d48b3b504602f3b744a2d",
            "3cdd6eba5dee4a5dae7fb0f314bf8e60",
            "ce8a1a496963486694f2b7a4255f83de",
            "cbc3952dbbef4dc2810d2dae7cb7f2e7",
            "c4c545c77dfb410eadf139ce48eaef11",
            "a3b5b8f4daa1478b8a7280abce17ac27",
            "bdaf858548ef436fa8a18fe92c68e6a1",
            "d798ac1ca4a148efae4d1820459dbd4a",
            "60f76696dbe24d40b714da30897873f2",
            "15e010c974324bd888f6210ea833e2e5",
            "a56d3444ce6d4e618c3d492df52e34c7",
            "c501c94b972743c2a11a7440f4a7fc70",
            "ec0f99a7f63e49e99bda2f6c39ba1f02",
            "15b687d803f04984bf857d5c79148800",
            "d0eaf5c129744fbfaed484016866bbfb",
            "cbc9571cb4ce486b8298cf5677eeb778",
            "c5e62ab632824e1f89da4c9689871f57",
            "41d3524188a94f059606b30ef3cd51dc",
            "1374890a24034a9db1adb1feedc09d4e",
            "82a7ecf81bc44c4b8bb7841637e11562",
            "1d2783b631e24e008204e7c1d00d4b37",
            "1c155ce3827d44dcbbdfc683182f9a9c",
            "4460b46e53744bdcb4cda4239b8c59a5",
            "96f3d4fe4754428fae14626270cace0e",
            "d32844c2917a49ea8948fd8f1025a170",
            "1d6608727fda46a38d7cd3f5372b45d1",
            "c007d59c98a542f5bced5b21cf519663",
            "5ee0d65279fc496d9972ada02d4815b6",
            "3f4f3e2a470e4c07895034e2ca0c9654",
            "050b8cc9d853459299ce82f86e7c7407",
            "86515aa2683e45e08789c786949f60e7",
            "0402c5acee1c461fb21b54a9079b700c",
            "e5495731e9d141d8832e5638922dd8f2",
            "9ff0a0a8295b4759b3d384f878c91df2",
            "31d3e8af7bf242459d3159a7282e4219"
          ]
        },
        "outputId": "f9446aa9-1064-4a4e-da94-18ebff8d8468"
      },
      "source": [
        "model_path = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "sentiment_task = pipeline(\"sentiment-analysis\", model=model_path, tokenizer=model_path)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1734ee4bd0dd425b8be9c8a69f5ce76b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "27ab61b2598e4998a38b28efb7e62db2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "47a78b2da9b946838468386922e99a3b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ce8a1a496963486694f2b7a4255f83de"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "15b687d803f04984bf857d5c79148800"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d32844c2917a49ea8948fd8f1025a170"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "efVw7pYkE39C",
        "outputId": "51efb680-970a-4b61-a0c3-f1486257c5a9"
      },
      "source": [
        "sentiment_task(\"Huggingface es lo mejor! Awesome library 🤗😎\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'positive', 'score': 0.9343641400337219}]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iOFaYztUBIDZ"
      },
      "source": [
        "# 3 - Experiment on UMSAB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTgvy8dh8JFL"
      },
      "source": [
        "## Fetch dataset (Spanish)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        },
        "id": "pN2Vy0qB8K7M",
        "outputId": "df0176ed-a816-4e79-d1be-1facffa3e83b"
      },
      "source": [
        "language = 'spanish'\n",
        "\n",
        "files = \"\"\"test_labels.txt\n",
        "test_text.txt\n",
        "train_labels.txt\n",
        "train_text.txt\n",
        "val_labels.txt\n",
        "val_text.txt\"\"\".split('\\n')\n",
        "\n",
        "def fetch_data(language, files):\n",
        " dataset = defaultdict(list)\n",
        " for infile in files:\n",
        "   thisdata = infile.split('/')[-1].replace('.txt','')\n",
        "   dataset_url = f\"https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/{language}/{infile}\"\n",
        "   print(f'Fetching from {dataset_url}')\n",
        "   with urllib.request.urlopen(dataset_url) as f:\n",
        "     for line in f:\n",
        "       if thisdata.endswith('labels'):\n",
        "         dataset[thisdata].append(int(line.strip().decode('utf-8')))\n",
        "       else:\n",
        "         dataset[thisdata].append(line.strip().decode('utf-8'))\n",
        " return dataset\n",
        "\n",
        "dataset = fetch_data(language, files)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fetching from https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/spanish/test_labels.txt\n",
            "Fetching from https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/spanish/test_text.txt\n",
            "Fetching from https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/spanish/train_labels.txt\n",
            "Fetching from https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/spanish/train_text.txt\n",
            "Fetching from https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/spanish/val_labels.txt\n",
            "Fetching from https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/spanish/val_text.txt\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xAkzQJBz83kN",
        "outputId": "f07be91a-8d35-4d66-a4a9-a403e1834ad4"
      },
      "source": [
        "dataset['train_text'][:3],dataset['train_labels'][:3]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(['estoy hasta el ojete de que me digáis que tengo cara de mala leche',\n",
              "  '@user Por?  Tenía pensado verla después de la segunda de Daredevil',\n",
              "  'Esto de estar feliz mola'],\n",
              " [0, 1, 2])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1IRg7RnGF9Me"
      },
      "source": [
        "## Run full experiment"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y11tfLcrF_nu"
      },
      "source": [
        "# load multilingual sentiment classifier\n",
        "CUDA = True # set to true if using GPU (Runtime -> Change runtime Type -> GPU)\n",
        "BATCH_SIZE = 32\n",
        "MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL)\n",
        "config = AutoConfig.from_pretrained(MODEL)\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "if CUDA:\n",
        "  model = model.to('cuda')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KV9DAkkwIez9"
      },
      "source": [
        "# helper functions\n",
        "def preprocess(corpus):\n",
        "  outcorpus = []\n",
        "  for text in corpus:\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    new_text = \" \".join(new_text)\n",
        "    outcorpus.append(new_text)\n",
        "  return outcorpus\n",
        "\n",
        "def predict(text, cuda):\n",
        "  text = preprocess(text)\n",
        "  encoded_input = tokenizer(text, return_tensors='pt', padding = True, truncation = True)\n",
        "  if cuda:\n",
        "    encoded_input.to('cuda')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0].detach().cpu().numpy()\n",
        "  else:\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0].detach().numpy()\n",
        "\n",
        "  scores = softmax(scores, axis=-1)\n",
        "  return scores"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_o7MTs4OPwS",
        "outputId": "51b14441-e94f-412e-9368-49a071b37361"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "dl = DataLoader(dataset['test_text'], batch_size=BATCH_SIZE)\n",
        "all_preds = []\n",
        "for idx,batch in enumerate(dl):\n",
        "  if idx % 10 == 0:\n",
        "    print('Batch ',idx+1,' of ',len(dl))\n",
        "  text = preprocess(batch)\n",
        "  scores = predict(text, CUDA)\n",
        "  preds = np.argmax(scores, axis=-1)\n",
        "  all_preds.extend(preds)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  1  of  28\n",
            "Batch  11  of  28\n",
            "Batch  21  of  28\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogWrPLWKPIG6",
        "outputId": "da0a212f-687f-48f4-aa34-db258ff48fa0"
      },
      "source": [
        "print(classification_report(dataset['test_labels'], all_preds))"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.70      0.80      0.75       290\n",
            "           1       0.62      0.55      0.58       290\n",
            "           2       0.75      0.74      0.75       290\n",
            "\n",
            "    accuracy                           0.70       870\n",
            "   macro avg       0.69      0.70      0.69       870\n",
            "weighted avg       0.69      0.70      0.69       870\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v0bWMH-l4xa4"
      },
      "source": [],
      "execution_count": 14,
      "outputs": []
    }
  ]
}
