{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.9"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaoDr57T74ag"
      },
      "source": [
        "## Installs and imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXC3dguV3cDb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76572da7-6710-43bb-f097-884fc1d4445d"
      },
      "source": [
        "!pip install --upgrade pip\n",
        "!pip install sentencepiece\n",
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.0.1-py3-none-any.whl.metadata (3.7 kB)\n",
            "Downloading pip-25.0.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.0.1\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (0.2.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.30.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.13.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YWLw1TOn6P_u"
      },
      "source": [
        "from transformers import AutoTokenizer, AutoModel, AutoConfig\n",
        "from transformers import AutoModelForSequenceClassification\n",
        "from torch.utils.data import DataLoader\n",
        "import numpy as np\n",
        "from scipy.special import softmax"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhakJgjlIvC0"
      },
      "source": [
        "## Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0aP-pGTWHFH7"
      },
      "source": [
        "def preprocess(corpus):\n",
        "  outcorpus = []\n",
        "  for text in corpus:\n",
        "    new_text = []\n",
        "    for t in text.split(\" \"):\n",
        "        t = '@user' if t.startswith('@') and len(t) > 1 else t\n",
        "        t = 'http' if t.startswith('http') else t\n",
        "        new_text.append(t)\n",
        "    new_text = \" \".join(new_text)\n",
        "    outcorpus.append(new_text)\n",
        "  return outcorpus"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VqnM48t2MtLG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e3d98932-8681-448c-bfb0-dabefc7f225e"
      },
      "source": [
        "!wget https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/all/test_text.txt"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-04-14 22:29:33--  https://raw.githubusercontent.com/cardiffnlp/xlm-t/main/data/sentiment/all/test_text.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.110.133, 185.199.109.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 654172 (639K) [text/plain]\n",
            "Saving to: ‘test_text.txt’\n",
            "\n",
            "test_text.txt       100%[===================>] 638.84K  --.-KB/s    in 0.05s   \n",
            "\n",
            "2025-04-14 22:29:34 (11.6 MB/s) - ‘test_text.txt’ saved [654172/654172]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tGWH45yrIkOH"
      },
      "source": [
        "dataset_path = './test_text.txt'\n",
        "dataset = open(dataset_path).read().split('\\n')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mTdC_qDhLO-I",
        "outputId": "db4d218e-8646-407a-d459-a75a5b80a454"
      },
      "source": [
        "# this is a dataset in 8 different languages\n",
        "for example in [0,870,1740,2610,3480,4350,5220,6090]:\n",
        "  print(dataset[example])"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نوال الزغبي (الشاب خالد ليس عالمي) هههههههه أتفرجي على ها الفيديو يا مبتدئة http vía @user\n",
            "Trying to have a conversation with my dad about vegetarianism is the most pointless infuriating thing ever #caveman \n",
            "Royal: le président n'aime pas les pauvres? \"c'est n'importe quoi\" http …\n",
            "@user korrekt! Verstehe sowas nicht...\n",
            "CONGRESS na ye party kabhi bani hoti na india ka partition hota nd na hi humari country itni khokhli hoti   @ \n",
            "@user @user Ma Ferrero? il compagno Ferrero? ma il suo partito esiste ancora? allora stiamo proprio frecati !!!\n",
            "todos os meus favoritos na prova de eliminação #MasterChefBR\n",
            "@user jajajaja dale, hacete la boluda vos jajaja igual a vos nunca se te puede tomar en serio te mando un abrazo desde Perú!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1S6wUeuqIsVI"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8v8HPwj6P_y",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 401,
          "referenced_widgets": [
            "9e9db2070b194a47bac850a84423fb9c",
            "612d39fe494f4f05ba94ebc27bc0f0cd",
            "57ecc9f82917412eb2bd1e3ff9092692",
            "9218e184e97342a29b5429b9c83f870c",
            "d8cfa34dfc174816b7b55395e0189a7c",
            "e470779ec58f432aa589d18eb9b6f03f",
            "010c94481abb46fc8cd6e223f379ea8e",
            "6412e383742c45e1ab72a48508bfdde7",
            "213b27bbace5404e9f8da0dfd8775157",
            "c8a9d199ef3a4ca8a0bad5274a51a52c",
            "d3aa76469a5e4263be22bf2a8c6897a1",
            "7aa805cee3ae42abafa5d14e9762180d",
            "f19a0b87701040ee8ce707dcca7d7e70",
            "aa8b189403be41789beb7a24357ca323",
            "356601c2703841b5a9da684eeffe1799",
            "6e578dfd29274031a1144f834d6f55c6",
            "32b347890d19459e924ec40368e22033",
            "0bfaf4f76b784695950b9b1694783eb5",
            "9b2a226b7d01449ba515fbb36d388be4",
            "f9d22c839186429fb99ada1848da6675",
            "8e691ddbed7b460c9558486105f133cf",
            "59b30fae47a7416d851c06e2825d7351",
            "068309218f674d5a869524a574513320",
            "87cb3a861eb94b8ba1bf0486e8857302",
            "989d75784a9042f19eedfe969ef2503b",
            "50ad94efd16d4234b662f0a1349ef17b",
            "ac8bddc4573445c4a2061f7e91733f7b",
            "c11f5f05efc6409fb7a8d02d948b3598",
            "cd3643001b664e25baec7f81fa57c77d",
            "9368f5527cde43debf3441047079b778",
            "94311eaba1e34ceb932e8dea6c248a57",
            "acb28296a38a407599b8e4fc90070623",
            "bbc996b8e9a040619525692e75cfb121",
            "35dca34c08f340f9b93ab62c7d68b8e9",
            "10b0368d9d4746adbfd7cb0f05c8da04",
            "645c4b2e6ca34a228dde65d22dea053b",
            "0b85659a97464d5cafd85e014ca98864",
            "92baf0099f0443c9baf45976e86b0ce3",
            "e7c2caad0ca1438ba80aaa1ddcce1740",
            "19df39a5f68f47f5961649b80c4f577b",
            "4bb4df8a403e4aa38c9986aeb3b04c7a",
            "615794ef878d4808b2b751b5b516e3b4",
            "81136caf03e74fa58dce67261725c915",
            "c57e01ceb1464e5cb1241e43b60479a0",
            "8bab41eea77d4ba38f4f297404fb8c9d",
            "bb2cd869641e40d4adb9b2969354964f",
            "f6fa19eaacde4143833fae2d7174e9a9",
            "93f0b15688524d8db3684de18e903efd",
            "423fd73354d344ebb9f096bbc21f60db",
            "6cabc2d3a41b4cf7964a51f57e7d311f",
            "4ace9fb2021f4e5681540939ae446fe2",
            "0ff7a61d144a4b1ebb41f9701a13aa43",
            "be1d0018097a4d96b70726e678ca4eca",
            "7eb6b627faf94d2faabfc9404c8201d8",
            "65d7fef866e8417791bda1040bce85ce"
          ]
        },
        "outputId": "880f531a-a7da-42fc-a265-54672aaeb162"
      },
      "source": [
        "CUDA = True # set to true if using GPU (Runtime -> Change runtime Type -> GPU)\n",
        "BATCH_SIZE = 32\n",
        "MODEL = \"cardiffnlp/twitter-xlm-roberta-base-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL, use_fast=True)\n",
        "config = AutoConfig.from_pretrained(MODEL) # used for id to label name\n",
        "model = AutoModelForSequenceClassification.from_pretrained(MODEL)\n",
        "if CUDA:\n",
        "  model = model.to('cuda')\n",
        "_ = model.eval()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json:   0%|          | 0.00/841 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9e9db2070b194a47bac850a84423fb9c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "sentencepiece.bpe.model:   0%|          | 0.00/5.07M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7aa805cee3ae42abafa5d14e9762180d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "special_tokens_map.json:   0%|          | 0.00/150 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "068309218f674d5a869524a574513320"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "pytorch_model.bin:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "35dca34c08f340f9b93ab62c7d68b8e9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n",
            "WARNING:huggingface_hub.file_download:Xet Storage is enabled for this repo, but the 'hf_xet' package is not installed. Falling back to regular HTTP download. For better performance, install the package with: `pip install huggingface_hub[hf_xet]` or `pip install hf_xet`\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/1.11G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8bab41eea77d4ba38f4f297404fb8c9d"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9fsx8PPInt-"
      },
      "source": [
        "## Forward"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c-Bhdf7cGsIX"
      },
      "source": [
        "def forward(text, cuda=True):\n",
        "  text = preprocess(text)\n",
        "  encoded_input = tokenizer(text, return_tensors='pt', padding=True, truncation=True)\n",
        "  if cuda:\n",
        "    encoded_input.to('cuda')\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0].detach().cpu().numpy()\n",
        "  else:\n",
        "    output = model(**encoded_input)\n",
        "    scores = output[0].detach().numpy()\n",
        "\n",
        "  scores = softmax(scores, axis=-1)\n",
        "  return scores"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eRCMvwm7GsK3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3e7fa220-edaf-4ec4-e06e-c730d195f92f"
      },
      "source": [
        "dl = DataLoader(dataset, batch_size=BATCH_SIZE)\n",
        "all_preds = []\n",
        "for idx,batch in enumerate(dl):\n",
        "  print('Batch ',idx+1,' of ',len(dl))\n",
        "  text = preprocess(batch)\n",
        "  scores = forward(text, cuda=CUDA)\n",
        "  preds = np.argmax(scores, axis=-1)\n",
        "  all_preds.extend(preds)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Batch  1  of  218\n",
            "Batch  2  of  218\n",
            "Batch  3  of  218\n",
            "Batch  4  of  218\n",
            "Batch  5  of  218\n",
            "Batch  6  of  218\n",
            "Batch  7  of  218\n",
            "Batch  8  of  218\n",
            "Batch  9  of  218\n",
            "Batch  10  of  218\n",
            "Batch  11  of  218\n",
            "Batch  12  of  218\n",
            "Batch  13  of  218\n",
            "Batch  14  of  218\n",
            "Batch  15  of  218\n",
            "Batch  16  of  218\n",
            "Batch  17  of  218\n",
            "Batch  18  of  218\n",
            "Batch  19  of  218\n",
            "Batch  20  of  218\n",
            "Batch  21  of  218\n",
            "Batch  22  of  218\n",
            "Batch  23  of  218\n",
            "Batch  24  of  218\n",
            "Batch  25  of  218\n",
            "Batch  26  of  218\n",
            "Batch  27  of  218\n",
            "Batch  28  of  218\n",
            "Batch  29  of  218\n",
            "Batch  30  of  218\n",
            "Batch  31  of  218\n",
            "Batch  32  of  218\n",
            "Batch  33  of  218\n",
            "Batch  34  of  218\n",
            "Batch  35  of  218\n",
            "Batch  36  of  218\n",
            "Batch  37  of  218\n",
            "Batch  38  of  218\n",
            "Batch  39  of  218\n",
            "Batch  40  of  218\n",
            "Batch  41  of  218\n",
            "Batch  42  of  218\n",
            "Batch  43  of  218\n",
            "Batch  44  of  218\n",
            "Batch  45  of  218\n",
            "Batch  46  of  218\n",
            "Batch  47  of  218\n",
            "Batch  48  of  218\n",
            "Batch  49  of  218\n",
            "Batch  50  of  218\n",
            "Batch  51  of  218\n",
            "Batch  52  of  218\n",
            "Batch  53  of  218\n",
            "Batch  54  of  218\n",
            "Batch  55  of  218\n",
            "Batch  56  of  218\n",
            "Batch  57  of  218\n",
            "Batch  58  of  218\n",
            "Batch  59  of  218\n",
            "Batch  60  of  218\n",
            "Batch  61  of  218\n",
            "Batch  62  of  218\n",
            "Batch  63  of  218\n",
            "Batch  64  of  218\n",
            "Batch  65  of  218\n",
            "Batch  66  of  218\n",
            "Batch  67  of  218\n",
            "Batch  68  of  218\n",
            "Batch  69  of  218\n",
            "Batch  70  of  218\n",
            "Batch  71  of  218\n",
            "Batch  72  of  218\n",
            "Batch  73  of  218\n",
            "Batch  74  of  218\n",
            "Batch  75  of  218\n",
            "Batch  76  of  218\n",
            "Batch  77  of  218\n",
            "Batch  78  of  218\n",
            "Batch  79  of  218\n",
            "Batch  80  of  218\n",
            "Batch  81  of  218\n",
            "Batch  82  of  218\n",
            "Batch  83  of  218\n",
            "Batch  84  of  218\n",
            "Batch  85  of  218\n",
            "Batch  86  of  218\n",
            "Batch  87  of  218\n",
            "Batch  88  of  218\n",
            "Batch  89  of  218\n",
            "Batch  90  of  218\n",
            "Batch  91  of  218\n",
            "Batch  92  of  218\n",
            "Batch  93  of  218\n",
            "Batch  94  of  218\n",
            "Batch  95  of  218\n",
            "Batch  96  of  218\n",
            "Batch  97  of  218\n",
            "Batch  98  of  218\n",
            "Batch  99  of  218\n",
            "Batch  100  of  218\n",
            "Batch  101  of  218\n",
            "Batch  102  of  218\n",
            "Batch  103  of  218\n",
            "Batch  104  of  218\n",
            "Batch  105  of  218\n",
            "Batch  106  of  218\n",
            "Batch  107  of  218\n",
            "Batch  108  of  218\n",
            "Batch  109  of  218\n",
            "Batch  110  of  218\n",
            "Batch  111  of  218\n",
            "Batch  112  of  218\n",
            "Batch  113  of  218\n",
            "Batch  114  of  218\n",
            "Batch  115  of  218\n",
            "Batch  116  of  218\n",
            "Batch  117  of  218\n",
            "Batch  118  of  218\n",
            "Batch  119  of  218\n",
            "Batch  120  of  218\n",
            "Batch  121  of  218\n",
            "Batch  122  of  218\n",
            "Batch  123  of  218\n",
            "Batch  124  of  218\n",
            "Batch  125  of  218\n",
            "Batch  126  of  218\n",
            "Batch  127  of  218\n",
            "Batch  128  of  218\n",
            "Batch  129  of  218\n",
            "Batch  130  of  218\n",
            "Batch  131  of  218\n",
            "Batch  132  of  218\n",
            "Batch  133  of  218\n",
            "Batch  134  of  218\n",
            "Batch  135  of  218\n",
            "Batch  136  of  218\n",
            "Batch  137  of  218\n",
            "Batch  138  of  218\n",
            "Batch  139  of  218\n",
            "Batch  140  of  218\n",
            "Batch  141  of  218\n",
            "Batch  142  of  218\n",
            "Batch  143  of  218\n",
            "Batch  144  of  218\n",
            "Batch  145  of  218\n",
            "Batch  146  of  218\n",
            "Batch  147  of  218\n",
            "Batch  148  of  218\n",
            "Batch  149  of  218\n",
            "Batch  150  of  218\n",
            "Batch  151  of  218\n",
            "Batch  152  of  218\n",
            "Batch  153  of  218\n",
            "Batch  154  of  218\n",
            "Batch  155  of  218\n",
            "Batch  156  of  218\n",
            "Batch  157  of  218\n",
            "Batch  158  of  218\n",
            "Batch  159  of  218\n",
            "Batch  160  of  218\n",
            "Batch  161  of  218\n",
            "Batch  162  of  218\n",
            "Batch  163  of  218\n",
            "Batch  164  of  218\n",
            "Batch  165  of  218\n",
            "Batch  166  of  218\n",
            "Batch  167  of  218\n",
            "Batch  168  of  218\n",
            "Batch  169  of  218\n",
            "Batch  170  of  218\n",
            "Batch  171  of  218\n",
            "Batch  172  of  218\n",
            "Batch  173  of  218\n",
            "Batch  174  of  218\n",
            "Batch  175  of  218\n",
            "Batch  176  of  218\n",
            "Batch  177  of  218\n",
            "Batch  178  of  218\n",
            "Batch  179  of  218\n",
            "Batch  180  of  218\n",
            "Batch  181  of  218\n",
            "Batch  182  of  218\n",
            "Batch  183  of  218\n",
            "Batch  184  of  218\n",
            "Batch  185  of  218\n",
            "Batch  186  of  218\n",
            "Batch  187  of  218\n",
            "Batch  188  of  218\n",
            "Batch  189  of  218\n",
            "Batch  190  of  218\n",
            "Batch  191  of  218\n",
            "Batch  192  of  218\n",
            "Batch  193  of  218\n",
            "Batch  194  of  218\n",
            "Batch  195  of  218\n",
            "Batch  196  of  218\n",
            "Batch  197  of  218\n",
            "Batch  198  of  218\n",
            "Batch  199  of  218\n",
            "Batch  200  of  218\n",
            "Batch  201  of  218\n",
            "Batch  202  of  218\n",
            "Batch  203  of  218\n",
            "Batch  204  of  218\n",
            "Batch  205  of  218\n",
            "Batch  206  of  218\n",
            "Batch  207  of  218\n",
            "Batch  208  of  218\n",
            "Batch  209  of  218\n",
            "Batch  210  of  218\n",
            "Batch  211  of  218\n",
            "Batch  212  of  218\n",
            "Batch  213  of  218\n",
            "Batch  214  of  218\n",
            "Batch  215  of  218\n",
            "Batch  216  of  218\n",
            "Batch  217  of  218\n",
            "Batch  218  of  218\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nYzTpKcrGsNH",
        "outputId": "ec1f08b9-6645-4785-851b-28cd494df6a0"
      },
      "source": [
        "# this is a dataset in 8 different languages\n",
        "for example in [0,870,1740,2610,3480,4350,5220,6090]:\n",
        "  pred = all_preds[example]\n",
        "  print(dataset[example], '--->', config.id2label[pred])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "نوال الزغبي (الشاب خالد ليس عالمي) هههههههه أتفرجي على ها الفيديو يا مبتدئة http vía @user ---> neutral\n",
            "Trying to have a conversation with my dad about vegetarianism is the most pointless infuriating thing ever #caveman  ---> negative\n",
            "Royal: le président n'aime pas les pauvres? \"c'est n'importe quoi\" http … ---> negative\n",
            "@user korrekt! Verstehe sowas nicht... ---> negative\n",
            "CONGRESS na ye party kabhi bani hoti na india ka partition hota nd na hi humari country itni khokhli hoti   @  ---> negative\n",
            "@user @user Ma Ferrero? il compagno Ferrero? ma il suo partito esiste ancora? allora stiamo proprio frecati !!! ---> negative\n",
            "todos os meus favoritos na prova de eliminação #MasterChefBR ---> positive\n",
            "@user jajajaja dale, hacete la boluda vos jajaja igual a vos nunca se te puede tomar en serio te mando un abrazo desde Perú! ---> neutral\n"
          ]
        }
      ]
    }
  ]
}
